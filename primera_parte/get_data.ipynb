{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping IMSSS\n",
    "\n",
    "### Web Scrapping de la página general por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardamos el url base para poder concatenar los links de los archivos\n",
    "base_url = \"http://datos.imss.gob.mx\"\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(\"http://datos.imss.gob.mx/dataset\")\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\") \n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", {'data-format':'csv'}, href=True)\n",
    "# Se guardaran los links de los archivos que nos interesan por año\n",
    "links = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for link in spans:\n",
    "    href = link.get(\"href\")\n",
    "    if '/dataset/asg-' in href:\n",
    "        condicion = int(re.findall(r'\\d{4}', href)[0])\n",
    "        if condicion >= 2006 and condicion < 2023:\n",
    "            links.append(base_url + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripción de fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea y abre el archivo donde se excribe la información de la referncia\n",
    "file = open('..\\descripciones\\imss_descripcion.txt', 'w+',encoding='utf-8')\n",
    "file.write('Datos del IMSS\\n')\n",
    "for link in links:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    file.write('Enlace de archivos por mes: '+link+'\\n')\n",
    "    # Tabla de referencias\n",
    "    table = soup.find('table', class_= 'field-group-format group_additional')\n",
    "    # Moviendose por filas\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        if row:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            # Extrae el texto de las celdas y los concatena\n",
    "            file.write(': '.join(cell.get_text(strip=True) for cell in cells))\n",
    "            file.write('\\n')\n",
    "# Agrega la fecha en la que se corrió el script que extrajo los datos\n",
    "fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "file.write('\\nFecha de descarga: '+fecha_actual+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping parte 2 por meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Por cada año, guardaremos los links de los archivos que nos interesan por meses\n",
    "links_2 = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links)):\n",
    "    r_2 = requests.get(links[i])\n",
    "    soup_2 = BeautifulSoup(r_2.content, \"html.parser\")\n",
    "    spans_2 = soup_2.find_all(\"a\", {'property':'dcat:accessURL'}, href=True)\n",
    "    for link in spans_2:\n",
    "        links_2.append(base_url + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos una lista para guardar los dataframes\n",
    "df = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links_2)):\n",
    "    r_3 = requests.get(links_2[i])\n",
    "    soup_3 = BeautifulSoup(r_3.content, \"html.parser\")\n",
    "    spans_3 = soup_3.find_all(\"a\", href=True)\n",
    "    for link in spans_3:\n",
    "        if '.csv' in link.get(\"href\"):\n",
    "            df.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(110692 bytes read, 233828188 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rodolfo Jaramillo\\Documents\\IngCaract\\Proyecto_Trabajos_Generados\\primera_parte\\get_data.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_combined \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Leer el archivo CSV y especificar el tipo de datos adecuado (si es necesario)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df_csv \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(df[i], encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlatin-1\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcolumna_con_tipo_mezclado\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mstr\u001b[39;49m}, low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mrango_uma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mta\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mteu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtpu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtpc\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mta_sal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mteu_sal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtec_sal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtpu_sal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtpc_sal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmasa_sal_ta\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mmasa_sal_teu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmasa_sal_tec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmasa_sal_tpu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmasa_sal_tpc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msector_economico_1\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rodolfo%20Jaramillo/Documents/IngCaract/Proyecto_Trabajos_Generados/primera_parte/get_data.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Usar copy() para evitar SettingWithCopyWarning\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    715\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    717\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    719\u001b[0m     path_or_buf,\n\u001b[0;32m    720\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    721\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    722\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    723\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    724\u001b[0m )\n\u001b[0;32m    726\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    727\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:377\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    375\u001b[0m             \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[0;32m    376\u001b[0m             compression \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 377\u001b[0m         reader \u001b[39m=\u001b[39m BytesIO(req\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m IOArgs(\n\u001b[0;32m    379\u001b[0m         filepath_or_buffer\u001b[39m=\u001b[39mreader,\n\u001b[0;32m    380\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m         mode\u001b[39m=\u001b[39mfsspec_mode,\n\u001b[0;32m    384\u001b[0m     )\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\http\\client.py:481\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[0;32m    482\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m    483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Program Files\\Python\\Python311\\Lib\\http\\client.py:632\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m--> 632\u001b[0m     \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n\u001b[0;32m    633\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(110692 bytes read, 233828188 more expected)"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame vacío para almacenar los datos combinados\n",
    "df_combined = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Leer el archivo CSV y especificar el tipo de datos adecuado (si es necesario)\n",
    "    df_csv = pd.read_csv(df[i], encoding=\"latin-1\", sep=\"|\", dtype={\"columna_con_tipo_mezclado\": str}, low_memory=False)\n",
    "\n",
    "    columns = ['rango_uma', 'ta', 'teu', 'tec', 'tpu', 'tpc',\n",
    "               'ta_sal', 'teu_sal', 'tec_sal', 'tpu_sal', 'tpc_sal', 'masa_sal_ta',\n",
    "               'masa_sal_teu', 'masa_sal_tec', 'masa_sal_tpu', 'masa_sal_tpc', \"sector_economico_1\"]\n",
    "\n",
    "    # Usar copy() para evitar SettingWithCopyWarning\n",
    "    df_sonora = df_csv.loc[(df_csv[\"sector_economico_1\"] == 0) & (df_csv[\"cve_entidad\"] == 26)].copy()\n",
    "\n",
    "    # Eliminar columnas\n",
    "    df_sonora.drop(columns, inplace=True, axis=1, errors='ignore')\n",
    "\n",
    "    # Extraer año y mes de la URL\n",
    "    df_sonora['año'] = re.findall(r'\\d{4}', df[i])[0]\n",
    "    df_sonora['mes'] = re.findall(r'-(\\d{2})-', df[i])[0]\n",
    "\n",
    "    # Filtrar por sector_economico_2\n",
    "    df_append = df_sonora[df_sonora[\"sector_economico_2\"].isin([1, 2, 4])]\n",
    "\n",
    "    # Usar pd.concat en lugar de append (sin ignore_index=True)\n",
    "    df_combined = pd.concat([df_combined, df_append])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "df_combined.to_csv('..\\data\\df_imss.csv') \n",
    "# files.download('df_imss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario y catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'delegación-subdelegación' se ha guardado en 'imss_delegación-subdelegación.csv'\n",
      "La hoja 'entidad-municipio' se ha guardado en 'imss_entidad-municipio.csv'\n",
      "La hoja 'sector 1' se ha guardado en 'imss_sector_1.csv'\n",
      "La hoja 'sector 2' se ha guardado en 'imss_sector_2.csv'\n",
      "La hoja 'sector 4' se ha guardado en 'imss_sector_4.csv'\n",
      "La hoja 'Tamaño de registro patronal' se ha guardado en 'imss_Tamaño_de_registro_patronal.csv'\n",
      "La hoja 'sexo' se ha guardado en 'imss_sexo.csv'\n",
      "La hoja 'Rango edad' se ha guardado en 'imss_Rango_edad.csv'\n",
      "La hoja 'Rango salario' se ha guardado en 'imss_Rango_salario.csv'\n",
      "La hoja 'Rango UMA' se ha guardado en 'imss_Rango_UMA.csv'\n"
     ]
    }
   ],
   "source": [
    "dic_cat = pd.read_excel('http://datos.imss.gob.mx/sites/default/files/diccionario_de_datos_1.xlsx', sheet_name=None)\n",
    "\n",
    "for sheet_name, sheet_data in dic_cat.items():\n",
    "    if sheet_name == 'ejemplo archivo':\n",
    "        continue  # Ignorar esta hoja y pasar a la siguiente\n",
    "    elif sheet_name == 'Layout':\n",
    "        # Guarda el layout en un archivo de Excel\n",
    "        sheet_data = dic_cat['Layout']\n",
    "        sheet_data.to_excel('diccionario_IMSS.xlsx')       \n",
    "    else:\n",
    "        # Reemplaza los espacios en el nombre de la hoja con guiones bajos\n",
    "        nombre_archivo_csv = f\"imss_{sheet_name.replace(' ', '_')}.csv\"\n",
    "        # Guarda el archivo CSV en el directorio\n",
    "        if not os.path.exists('..\\data\\imss_cat'):\n",
    "                os.makedirs('..\\data\\imss_cat')\n",
    "        sheet_data.to_csv(f'..//data//imss_cat/{nombre_archivo_csv}')\n",
    "        print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para catálogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogos(tipo_catalogo, soup):\n",
    "    # Utilizar una expresión regular para buscar elementos <a> con 'title' que contiene 'Catalogos'\n",
    "    pattern = re.compile(r'Cat[aá]logos', re.IGNORECASE)  # La opción re.IGNORECASE hace que la búsqueda sea insensible a mayúsculas/minúsculas\n",
    "    enlace = soup.find_all(\"a\", {'title':pattern}, href=True)\n",
    "    for link in enlace:\n",
    "        href = link.get(\"href\")\n",
    "        df = pd.read_excel(href, sheet_name=None)\n",
    "        # Se guarda el archivo\n",
    "        nombre_archivo = os.path.join('..\\\\data\\\\', f\"cat_{tipo_catalogo}.xlsx\")\n",
    "        response = requests.get(href)\n",
    "        with open(nombre_archivo, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        # Iteramos a través de las hojas del archivo\n",
    "        for sheet_name, sheet_data in df.items():\n",
    "            # Genera un nombre de archivo CSV basado en el nombre de la hoja\n",
    "            nombre_archivo_csv = f\"{tipo_catalogo}_{sheet_name}.csv\"\n",
    "            # Se guarda en\n",
    "            if not os.path.exists(f'..\\data\\{tipo_catalogo}'):\n",
    "                os.makedirs(f'..\\data\\{tipo_catalogo}')\n",
    "            # Guarda el DataFrame de la hoja en un archivo CSV\n",
    "            sheet_data.to_csv(f'..\\\\data\\\\{tipo_catalogo}\\\\{nombre_archivo_csv}', index=False)\n",
    "            print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Agricultura (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el url\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-de-agricultura-sonora/1573'\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(url)\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Información de la fuente\n",
    "file = open('..\\\\descripciones\\\\agricultura_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'ClasificaciónCultivos' se ha guardado en 'Agricultura_ClasificaciónCultivos.csv'\n",
      "La hoja 'CiclosCultivos' se ha guardado en 'Agricultura_CiclosCultivos.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Agricultura_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Agricultura_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Agricultura_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Agricultura', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = [str(year) for year in range(2006,2023)]\n",
    "links_xlsx = []\n",
    "# Recorre la lista de años y busca enlaces con títulos que coincidan con cada año\n",
    "for year in years:\n",
    "    links = soup.find_all(\"a\", {'title': f'Producción agrícola {year}'}, href=True)\n",
    "    for link in links:\n",
    "        links_xlsx.append(link['href'])\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_agricultura = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_agricultura = pd.concat([df_agricultura, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "df_agricultura.to_csv('..\\data\\df_agricultura.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pecuaria (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-ganaderia-sonora/1581'\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "# Información de la fuente\n",
    "file = open('..\\\\descripciones\\\\pecuaria_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'Catálogo_Ganadería' se ha guardado en 'Ganaderia_Catálogo_Ganadería.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Ganaderia_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Ganaderia_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Ganaderia_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Ganaderia', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = [str(year) for year in range(2006,2023)]\n",
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pecuaria = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pecuaria = pd.concat([df_pecuaria, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "df_pecuaria.to_csv('..\\data\\df_pecuaria.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pesca (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-pesca-sonora/1582'\n",
    "\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Información de la fuente\n",
    "file = open('..\\\\descripciones\\\\pesca_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'Especie_Pesca' se ha guardado en 'Pesca_Especie_Pesca.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Pesca_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Pesca_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Pesca_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Pesca', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = [str(year) for year in range(2006,2023)]\n",
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pesca = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pesca = pd.concat([df_pesca, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "df_pesca.to_csv('..\\data\\df_pesca.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
