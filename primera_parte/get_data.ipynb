{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime \n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_actual = datetime.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping IMSSS\n",
    "\n",
    "### Web Scrapping de la página general por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardamos el url base para poder concatenar los links de los archivos\n",
    "base_url = \"http://datos.imss.gob.mx\"\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(\"http://datos.imss.gob.mx/dataset\")\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", {'data-format':'csv'}, href=True)\n",
    "# Se guardaran los links de los archivos que nos interesan por año\n",
    "links = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for link in spans:\n",
    "    href = link.get(\"href\")\n",
    "    if '/dataset/asg-' in href:\n",
    "        condicion = int(re.findall(r'\\d{4}', href)[0])\n",
    "        if condicion >= 2006 and condicion < 2023:\n",
    "            links.append(base_url + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripción de fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Crea y abre el archivo donde se excribe la información de la referncia\n",
    "ruta_archivo = os.path.join('..', 'descripciones', 'imss_descripcion.txt')\n",
    "with open(ruta_archivo, 'w+', encoding='utf-8') as file:\n",
    "    file.write('Datos del IMSS\\n')\n",
    "    for link in links:\n",
    "        time.sleep(5)\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        file.write('Enlace de archivos por mes: '+link+'\\n')\n",
    "        # Tabla de referencias\n",
    "        table = soup.find('table', class_= 'field-group-format group_additional')\n",
    "        # Moviendose por filas\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows[1:]:\n",
    "            if row:\n",
    "                cells = row.find_all(['th', 'td'])\n",
    "                # Extrae el texto de las celdas y los concatena\n",
    "                file.write(': '.join(cell.get_text(strip=True) for cell in cells))\n",
    "                file.write('\\n')\n",
    "    # Agrega la fecha en la que se corrió el script que extrajo los datos\n",
    "    file.write('\\nFecha de descarga: '+fecha_actual+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping parte 2 por meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Por cada año, guardaremos los links de los archivos que nos interesan por meses\n",
    "links_2 = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links)):\n",
    "    time.sleep(5)\n",
    "    r_2 = requests.get(links[i])\n",
    "    soup_2 = BeautifulSoup(r_2.content, \"html.parser\")\n",
    "    spans_2 = soup_2.find_all(\"a\", {'property':'dcat:accessURL'}, href=True)\n",
    "    for link in spans_2:\n",
    "        links_2.append(base_url + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos una lista para guardar los dataframes\n",
    "df = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links_2)):\n",
    "    time.sleep(5)\n",
    "    r_3 = requests.get(links_2[i])\n",
    "    soup_3 = BeautifulSoup(r_3.content, \"html.parser\")\n",
    "    spans_3 = soup_3.find_all(\"a\", href=True)\n",
    "    for link in spans_3:\n",
    "        if '.csv' in link.get(\"href\"):\n",
    "            df.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34140/1681092507.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_combined = pd.concat([df_combined, df_append])\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame vacío para almacenar los datos combinados\n",
    "df_combined = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Leer el archivo CSV y especificar el tipo de datos adecuado (si es necesario)\n",
    "    df_csv = pd.read_csv(df[i], encoding=\"latin-1\", sep=\"|\", dtype={\"columna_con_tipo_mezclado\": str}, low_memory=False)\n",
    "\n",
    "    columns = ['rango_uma', 'ta', 'teu', 'tec', 'tpu', 'tpc',\n",
    "               'ta_sal', 'teu_sal', 'tec_sal', 'tpu_sal', 'tpc_sal', 'masa_sal_ta',\n",
    "               'masa_sal_teu', 'masa_sal_tec', 'masa_sal_tpu', 'masa_sal_tpc', \"sector_economico_1\"]\n",
    "\n",
    "    # Usar copy() para evitar SettingWithCopyWarning\n",
    "    df_sonora = df_csv.loc[(df_csv[\"sector_economico_1\"] == 0) & (df_csv[\"cve_entidad\"] == 26)].copy()\n",
    "\n",
    "    # Eliminar columnas\n",
    "    df_sonora.drop(columns, inplace=True, axis=1, errors='ignore')\n",
    "\n",
    "    # Extraer año y mes de la URL\n",
    "    df_sonora['año'] = re.findall(r'\\d{4}', df[i])[0]\n",
    "    df_sonora['mes'] = re.findall(r'-(\\d{2})-', df[i])[0]\n",
    "\n",
    "    # Filtrar por sector_economico_2\n",
    "    df_append = df_sonora[df_sonora[\"sector_economico_2\"].isin([1, 2, 4])]\n",
    "\n",
    "    # Usar pd.concat en lugar de append (sin ignore_index=True)\n",
    "    df_combined = pd.concat([df_combined, df_append])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo = os.path.join('..', 'data')\n",
    "if not os.path.exists(ruta_archivo):\n",
    "    os.makedirs(ruta_archivo)\n",
    "ruta = os.path.join('..', 'data','df_imss.csv')\n",
    "df_combined.to_csv(ruta) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario y catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'delegación-subdelegación' se ha guardado en 'imss_delegación-subdelegación.csv'\n",
      "La hoja 'entidad-municipio' se ha guardado en 'imss_entidad-municipio.csv'\n",
      "La hoja 'sector 1' se ha guardado en 'imss_sector_1.csv'\n",
      "La hoja 'sector 2' se ha guardado en 'imss_sector_2.csv'\n",
      "La hoja 'sector 4' se ha guardado en 'imss_sector_4.csv'\n",
      "La hoja 'Tamaño de registro patronal' se ha guardado en 'imss_Tamaño_de_registro_patronal.csv'\n",
      "La hoja 'sexo' se ha guardado en 'imss_sexo.csv'\n",
      "La hoja 'Rango edad' se ha guardado en 'imss_Rango_edad.csv'\n",
      "La hoja 'Rango salario' se ha guardado en 'imss_Rango_salario.csv'\n",
      "La hoja 'Rango UMA' se ha guardado en 'imss_Rango_UMA.csv'\n"
     ]
    }
   ],
   "source": [
    "dic_cat = pd.read_excel('http://datos.imss.gob.mx/sites/default/files/diccionario_de_datos_1.xlsx', sheet_name=None)\n",
    "\n",
    "for sheet_name, sheet_data in dic_cat.items():\n",
    "    if sheet_name == 'ejemplo archivo':\n",
    "        continue  # Ignorar esta hoja y pasar a la siguiente\n",
    "    elif sheet_name == 'Layout':\n",
    "        # Guarda el layout en un archivo de Excel\n",
    "        sheet_data = dic_cat['Layout']\n",
    "        sheet_data.to_excel('diccionario_IMSS.xlsx')       \n",
    "    else:\n",
    "        # Reemplaza los espacios en el nombre de la hoja con guiones bajos\n",
    "        nombre_archivo_csv = f\"imss_{sheet_name.replace(' ', '_')}.csv\"\n",
    "        # Guarda el archivo CSV en el directorio\n",
    "        ruta_archivo = os.path.join('..','data','imss_cat')\n",
    "        if not os.path.exists(ruta_archivo):\n",
    "            os.makedirs(ruta_archivo)\n",
    "        ruta_archivo = os.path.join('..','data','imss_cat', nombre_archivo_csv)\n",
    "        sheet_data.to_csv(ruta_archivo)\n",
    "        print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para catálogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogos(tipo_catalogo, soup):\n",
    "    # Utilizar una expresión regular para buscar elementos <a> con 'title' que contiene 'Catalogos'\n",
    "    pattern = re.compile(r'Cat[aá]logos', re.IGNORECASE)  # La opción re.IGNORECASE hace que la búsqueda sea insensible a mayúsculas/minúsculas\n",
    "    enlace = soup.find_all(\"a\", {'title':pattern}, href=True)\n",
    "    for link in enlace:\n",
    "        href = link.get(\"href\")\n",
    "        df = pd.read_excel(href, sheet_name=None)\n",
    "        direc = os.path.join('..','data')\n",
    "        if not os.path.exists(direc):\n",
    "                os.makedirs(direc)\n",
    "        # Se guarda el archivo\n",
    "        nombre_archivo = os.path.join('..','data', f\"cat_{tipo_catalogo}.xlsx\")\n",
    "        response = requests.get(href)\n",
    "        with open(nombre_archivo, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        # Iteramos a través de las hojas del archivo\n",
    "        for sheet_name, sheet_data in df.items():\n",
    "            # Genera un nombre de archivo CSV basado en el nombre de la hoja\n",
    "            nombre_archivo_csv = f\"{tipo_catalogo}_{sheet_name}.csv\"\n",
    "            # Se guarda en\n",
    "            ruta = os.path.join('..', 'data',f'{tipo_catalogo}')\n",
    "            if not os.path.exists(ruta):\n",
    "                os.makedirs(ruta)\n",
    "            # Guarda el DataFrame de la hoja en un archivo CSV\n",
    "            ruta_archivo = os.path.join(ruta, nombre_archivo_csv)\n",
    "            sheet_data.to_csv(ruta_archivo, index = False)\n",
    "            print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Agricultura (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el url\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-de-agricultura-sonora/1573'\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(url)\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Información de la fuente\n",
    "ruta = os.path.join('..', 'descripciones','agricultura_descripcion.txt')\n",
    "file = open(ruta, 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'ClasificaciónCultivos' se ha guardado en 'Agricultura_ClasificaciónCultivos.csv'\n",
      "La hoja 'CiclosCultivos' se ha guardado en 'Agricultura_CiclosCultivos.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Agricultura_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Agricultura_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Agricultura_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Agricultura', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = [str(year) for year in range(2006,2021)]\n",
    "links_xlsx = []\n",
    "# Recorre la lista de años y busca enlaces con títulos que coincidan con cada año\n",
    "for year in years:\n",
    "    links = soup.find_all(\"a\", {'title': f'Producción agrícola {year}'}, href=True)\n",
    "    for link in links:\n",
    "        links_xlsx.append(link['href'])\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_agricultura = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_agricultura = pd.concat([df_agricultura, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "df_2021 = pd.read_excel('../data/extra/Agricultura Sonora 2021.xlsx')\n",
    "df_2022 = pd.read_excel('../data/extra/Agricultura Sonora 2022.xlsx')\n",
    "\n",
    "df_agricultura = pd.concat([df_agricultura, df_2021])\n",
    "df_agricultura = pd.concat([df_agricultura, df_2022])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "ruta = os.path.join('..','data','df_agricultura.csv')\n",
    "df_agricultura.to_csv(ruta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pecuaria (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-ganaderia-sonora/1581'\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "# Información de la fuente\n",
    "ruta = os.path.join('..', 'descripciones','pecuaria_descripcion.txt')\n",
    "file = open(ruta, 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'Catálogo_Ganadería' se ha guardado en 'Ganaderia_Catálogo_Ganadería.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Ganaderia_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Ganaderia_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Ganaderia_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Ganaderia', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = [str(year) for year in range(2006,2021)]\n",
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pecuaria = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pecuaria = pd.concat([df_pecuaria, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "df_2021 = pd.read_excel('../data/extra/Cierre pecuario 2021.xlsx')\n",
    "df_2022 = pd.read_excel('../data/extra/Cierre pecuario 2022.xlsx')\n",
    "\n",
    "df_pecuaria = pd.concat([df_pecuaria, df_2021])\n",
    "df_pecuaria = pd.concat([df_pecuaria, df_2022])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "ruta = os.path.join('..','data','df_pecuaria.csv')\n",
    "df_pecuaria.to_csv(ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pesca (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-pesca-sonora/1582'\n",
    "\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Información de la fuente\n",
    "ruta = os.path.join('..', 'descripciones','pesca_descripcion.txt')\n",
    "file = open(ruta, 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'Especie_Pesca' se ha guardado en 'Pesca_Especie_Pesca.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Pesca_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Pesca_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Pesca_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Pesca', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = [str(year) for year in range(2006,2021)]\n",
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pesca = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pesca = pd.concat([df_pesca, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "df_2021 = pd.read_excel('../data/extra/Datos pesca 2021.xlsx')\n",
    "df_2022 = pd.read_excel('../data/extra/Datos pesca 2022.xlsx')\n",
    "\n",
    "df_pesca = pd.concat([df_pesca, df_2021])\n",
    "df_pesca = pd.concat([df_pesca, df_2022])\n",
    "\n",
    "\n",
    "# Convertimos el df a csv\n",
    "ruta = os.path.join('..','data','df_pesca.csv')\n",
    "df_pesca.to_csv(ruta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
