{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping IMSSS\n",
    "\n",
    "### Web Scrapping de la página general por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardamos el url base para poder concatenar los links de los archivos\n",
    "base_url = \"http://datos.imss.gob.mx\"\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(\"http://datos.imss.gob.mx/dataset\")\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\") \n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", {'data-format':'csv'}, href=True)\n",
    "# Se guardaran los links de los archivos que nos interesan por año\n",
    "links = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for link in spans:\n",
    "    href = link.get(\"href\")\n",
    "    if '/dataset/asg-' in href:\n",
    "        condicion = int(re.findall(r'\\d{4}', href)[0])\n",
    "        if condicion >= 2018 and condicion < 2023:\n",
    "            links.append(base_url + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripción de fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea y abre el archivo donde se excribe la información de la referncia\n",
    "file = open('descripciones\\imss_descripcion.txt', 'w+',encoding='utf-8')\n",
    "file.write('Datos del IMSS\\n')\n",
    "for link in links:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    file.write('Enlace de archivos por mes: '+link+'\\n')\n",
    "    # Tabla de referencias\n",
    "    table = soup.find('table', class_= 'field-group-format group_additional')\n",
    "    # Moviendose por filas\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        if row:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            # Extrae el texto de las celdas y los concatena\n",
    "            file.write(': '.join(cell.get_text(strip=True) for cell in cells))\n",
    "            file.write('\\n')\n",
    "# Agrega la fecha en la que se corrió el script que extrajo los datos\n",
    "fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "file.write('\\nFecha de descarga: '+fecha_actual+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping parte 2 por meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Por cada año, guardaremos los links de los archivos que nos interesan por meses\n",
    "links_2 = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links)):\n",
    "    r_2 = requests.get(links[i])\n",
    "    soup_2 = BeautifulSoup(r_2.content, \"html.parser\")\n",
    "    spans_2 = soup_2.find_all(\"a\", {'property':'dcat:accessURL'}, href=True)\n",
    "    for link in spans_2:\n",
    "        links_2.append(base_url + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos una lista para guardar los dataframes\n",
    "df = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links_2)):\n",
    "    r_3 = requests.get(links_2[i])\n",
    "    soup_3 = BeautifulSoup(r_3.content, \"html.parser\")\n",
    "    spans_3 = soup_3.find_all(\"a\", href=True)\n",
    "    for link in spans_3:\n",
    "        if '.csv' in link.get(\"href\"):\n",
    "            df.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear un DataFrame vacío para almacenar los datos combinados\n",
    "df_combined = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Leer el archivo CSV y especificar el tipo de datos adecuado (si es necesario)\n",
    "    df_csv = pd.read_csv(df[i], encoding=\"latin-1\", sep=\"|\", dtype={\"columna_con_tipo_mezclado\": str}, low_memory=False)\n",
    "\n",
    "    columns = ['rango_uma', 'ta', 'teu', 'tec', 'tpu', 'tpc',\n",
    "               'ta_sal', 'teu_sal', 'tec_sal', 'tpu_sal', 'tpc_sal', 'masa_sal_ta',\n",
    "               'masa_sal_teu', 'masa_sal_tec', 'masa_sal_tpu', 'masa_sal_tpc']\n",
    "\n",
    "    # Usar copy() para evitar SettingWithCopyWarning\n",
    "    df_sonora = df_csv.loc[(df_csv[\"sector_economico_1\"] == 0) & (df_csv[\"cve_entidad\"] == 26)].copy()\n",
    "\n",
    "    # Eliminar columnas\n",
    "    df_sonora.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "    # Extraer año y mes de la URL\n",
    "    df_sonora['año'] = re.findall(r'\\d{4}', df[i])[0]\n",
    "    df_sonora['mes'] = re.findall(r'-(\\d{2})-', df[i])[0]\n",
    "\n",
    "    # Filtrar por sector_economico_2\n",
    "    df_append = df_sonora[df_sonora[\"sector_economico_2\"].isin([1, 2, 4])]\n",
    "\n",
    "    # Usar pd.concat en lugar de append (sin ignore_index=True)\n",
    "    df_combined = pd.concat([df_combined, df_append])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "df_combined.to_csv('data\\df_imss.csv') \n",
    "# files.download('df_imss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario y catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entidad-municipio\n",
      "sector 2\n",
      "delegación-subdelegación\n",
      "ejemplo archivo\n",
      "Layout\n",
      "sector 4\n",
      "La hoja 'delegación-subdelegación' se ha guardado en 'imss_delegación-subdelegación.csv'\n",
      "La hoja 'entidad-municipio' se ha guardado en 'imss_entidad-municipio.csv'\n",
      "La hoja 'sector 1' se ha guardado en 'imss_sector_1.csv'\n",
      "La hoja 'sector 2' se ha guardado en 'imss_sector_2.csv'\n",
      "La hoja 'sector 4' se ha guardado en 'imss_sector_4.csv'\n",
      "La hoja 'Tamaño de registro patronal' se ha guardado en 'imss_Tamaño_de_registro_patronal.csv'\n",
      "La hoja 'sexo' se ha guardado en 'imss_sexo.csv'\n",
      "La hoja 'Rango edad' se ha guardado en 'imss_Rango_edad.csv'\n",
      "La hoja 'Rango salario' se ha guardado en 'imss_Rango_salario.csv'\n",
      "La hoja 'Rango UMA' se ha guardado en 'imss_Rango_UMA.csv'\n"
     ]
    }
   ],
   "source": [
    "dic_cat = pd.read_excel('http://datos.imss.gob.mx/sites/default/files/diccionario_de_datos_1.xlsx', sheet_name=None)\n",
    "\n",
    "for sheet_name, sheet_data in dic_cat.items():\n",
    "    if sheet_name == 'ejemplo archivo':\n",
    "        continue  # Ignorar esta hoja y pasar a la siguiente\n",
    "    elif sheet_name == 'Layout':\n",
    "        # Guarda el layout en un archivo de Excel\n",
    "        sheet_data = dic_cat['Layout']\n",
    "        sheet_data.to_excel('diccionario_IMSS.xlsx')       \n",
    "    else:\n",
    "        # Reemplaza los espacios en el nombre de la hoja con guiones bajos\n",
    "        nombre_archivo_csv = f\"imss_{sheet_name.replace(' ', '_')}.csv\"\n",
    "        # Guarda el archivo CSV en el directorio 'data'\n",
    "        sheet_data.to_csv(f'data/{nombre_archivo_csv}')\n",
    "        print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para catálogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogos(tipo_catalogo, soup):\n",
    "    # Utilizar una expresión regular para buscar elementos <a> con 'title' que contiene 'Catalogos'\n",
    "    pattern = re.compile(r'Cat[aá]logos', re.IGNORECASE)  # La opción re.IGNORECASE hace que la búsqueda sea insensible a mayúsculas/minúsculas\n",
    "    enlace = soup.find_all(\"a\", {'title':pattern}, href=True)\n",
    "    for link in enlace:\n",
    "        href = link.get(\"href\")\n",
    "        df = pd.read_excel(href, sheet_name=None)\n",
    "        # Iteramos a través de las hojas del archivo\n",
    "        for sheet_name, sheet_data in df.items():\n",
    "            # Genera un nombre de archivo CSV basado en el nombre de la hoja\n",
    "            nombre_archivo_csv = f\"{tipo_catalogo}_{sheet_name}.csv\"\n",
    "            # Guarda el DataFrame de la hoja en un archivo CSV\n",
    "            sheet_data.to_csv(nombre_archivo_csv, index=False)\n",
    "            print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Agricultura (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el url\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-de-agricultura-sonora/1573'\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(url)\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Información de la fuente\n",
    "file = open('descripciones\\\\agricultura_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Agricultura', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "links_xlsx = []\n",
    "# Recorre la lista de años y busca enlaces con títulos que coincidan con cada año\n",
    "for year in years:\n",
    "    links = soup.find_all(\"a\", {'title': f'Producción agrícola {year}'}, href=True)\n",
    "    for link in links:\n",
    "        links_xlsx.append(link['href'])\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_agricultura = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_agricultura = pd.concat([df_agricultura, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "df_agricultura.to_csv('data\\df_agricultura.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pecuaria (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-ganaderia-sonora/1581'\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "# Información de la fuente\n",
    "file = open('descripciones\\\\pecuaria_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la función catalogos\n",
    "catalogos('Pecuaria', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pecuaria = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pecuaria = pd.concat([df_pecuaria, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "df_pecuaria.to_csv('data\\df_pecuaria.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pesca (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-pesca-sonora/1582'\n",
    "\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Información de la fuente\n",
    "file = open('descripciones\\\\pesca_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de años\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pesca = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pesca = pd.concat([df_pesca, pd.read_excel(links_xlsx[i])])\n",
    "\n",
    "# Convertimos el df a csv\n",
    "df_pesca.to_csv('data\\df_pesca.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
